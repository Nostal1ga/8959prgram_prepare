{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNrQ/GMmo+mzdBbsYvXbsm3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nostal1ga/8959prgram_prepare/blob/main/draft1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GAqxh9tsaxaU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor, matmul\n",
        "from torch.nn import (\n",
        "    BatchNorm1d, Dropout, Embedding, Linear, Module, ModuleList, ReLU, LeakyReLU,Sequential\n",
        ")\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import GNNBenchmarkDataset, Planetoid, ZINC\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import (\n",
        "    GINEConv, global_add_pool, MessagePassing, BatchNorm\n",
        ")\n",
        "from torch_geometric.nn.inits import reset\n",
        "from torch_geometric.nn.resolver import activation_resolver, normalization_resolver\n",
        "from torch_geometric.typing import Adj\n",
        "from torch_geometric.utils import degree, sort_edge_index, to_dense_batch\n",
        "\n",
        "from mamba_ssm import Mamba\n",
        "import inspect\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path, subset = '/temp', True\n",
        "\n",
        "transform = T.AddRandomWalkPE(walk_length=20, attr_name='pe')\n",
        "train_dataset = ZINC(path, subset=subset, split='train', pre_transform=transform)\n",
        "val_dataset = ZINC(path, subset=subset, split='val', pre_transform=transform)\n",
        "test_dataset = ZINC(path, subset=subset, split='test', pre_transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "sBY3QI5ykmAZ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MambaConv(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        conv: MessagePassing,\n",
        "        dropout: float = 0.0,\n",
        "        d_state: int = 16,\n",
        "        d_conv: int = 4,\n",
        "        shuffle_ind: int = 0,\n",
        "        norm: str = 'batch_norm'\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.channels = channels\n",
        "        self.conv = conv\n",
        "        self.dropout = dropout\n",
        "        self.shuffle_ind = shuffle_ind\n",
        "\n",
        "        self.self_attn = Mamba(\n",
        "                d_model=channels,\n",
        "                d_state=d_state,\n",
        "                d_conv=d_conv,\n",
        "                expand=2## Expansion factor\n",
        "        )\n",
        "\n",
        "        self.mlp = Sequential(\n",
        "            Linear(channels, channels * 2),\n",
        "            torch.nn.ReLU(),\n",
        "            Dropout(dropout),\n",
        "            Linear(channels * 2, channels),\n",
        "            Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        self.norm1, self.norm2, self.norm3 = [BatchNorm(channels) for _ in range(3)]\n",
        "        self.norm_with_batch = 'batch' in inspect.signature(self.norm1.forward).parameters\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        if self.conv:\n",
        "            self.conv.reset_parameters()\n",
        "        self.self_attn._reset_parameters()\n",
        "        self.mlp.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
        "        for norm in [self.norm1, self.norm2, self.norm3]:\n",
        "            norm.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None, **kwargs):\n",
        "        hs = []\n",
        "\n",
        "        if self.conv is not None:\n",
        "            conv_output = self.conv(x, edge_index, **kwargs)\n",
        "            conv_output = F.dropout(conv_output, p=self.dropout, training=self.training)\n",
        "            conv_output = conv_output + x  # Residual connection\n",
        "            conv_output = self._apply_norm(conv_output, self.norm1, batch)\n",
        "            hs.append(conv_output)\n",
        "\n",
        "        # Process self-attention\n",
        "        if self.shuffle_ind == 0:\n",
        "            h, mask = to_dense_batch(x, batch)\n",
        "            attn_output = self.self_attn(h)[mask]\n",
        "        else:\n",
        "            attn_outputs = [self._process_shuffled_self_attn(x, batch) for _ in range(self.shuffle_ind)]\n",
        "            attn_output = sum(attn_outputs) / self.shuffle_ind\n",
        "\n",
        "        attn_output = F.dropout(attn_output, p=self.dropout, training=self.training)\n",
        "        attn_output = attn_output + x  # Residual connection\n",
        "        attn_output = self._apply_norm(attn_output, self.norm2, batch)\n",
        "        hs.append(attn_output)\n",
        "\n",
        "        # Combine all outputs\n",
        "        combined_output = sum(hs)\n",
        "        final_output = self.mlp(combined_output) + combined_output  # Pass through MLP and add residual\n",
        "        final_output = self._apply_norm(final_output, self.norm3, batch)\n",
        "\n",
        "        return final_output\n",
        "\n",
        "    def _apply_norm(self, input_tensor, norm_layer, batch):\n",
        "        #\"Applies the given normalization layer\"\n",
        "        if norm_layer is not None:\n",
        "            if self.norm_with_batch:\n",
        "                return norm_layer(input_tensor, batch=batch)\n",
        "            else:\n",
        "                return norm_layer(input_tensor)\n",
        "        return input_tensor\n",
        "\n",
        "    def _process_shuffled_self_attn(self, x, batch):\n",
        "        #\"Processes self-attention for shuffled inputs.\"\n",
        "        permuted_indices = self.permute_within_batch(x, batch)\n",
        "        dense_x, mask = to_dense_batch(x[permuted_indices], batch)\n",
        "        return self.self_attn(dense_x)[mask][permuted_indices]\n",
        "\n",
        "    def permute_within_batch(self,x, batch):\n",
        "        permuted_indices = torch.cat([\n",
        "            indices[torch.randperm(len(indices))]\n",
        "            for batch_index in torch.unique(batch)\n",
        "            for indices in [(batch == batch_index).nonzero().squeeze()]\n",
        "        ])\n",
        "        return permuted_indices\n",
        "\n",
        "    def message(self, x_j, edge_weight):\n",
        "        return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t, x) :\n",
        "        return matmul(adj_t, x, reduce=self.aggr)\n",
        "\n",
        "    def __repr__(self):\n",
        "      conv_type = self.conv.__class__.__name__ if self.conv else 'None'\n",
        "      return (f\"{self.__class__.__name__}(\"\n",
        "              f\"channels={self.channels}, \"\n",
        "              f\"d_state={self.d_state}, \"\n",
        "              f\"d_conv={self.d_conv}, \")"
      ],
      "metadata": {
        "id": "ZAG3A9fAfiiW"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphModel(torch.nn.Module):\n",
        "    def __init__(self, channels: int, pe_dim: int, num_layers: int,shuffle_ind: int, d_state: int, d_conv: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.node_emb = Embedding(28, channels - pe_dim)\n",
        "        self.pe_lin = Linear(20, pe_dim)\n",
        "        self.pe_norm = BatchNorm1d(20)\n",
        "        self.edge_emb = Embedding(4, channels)\n",
        "        self.shuffle_ind = shuffle_ind\n",
        "\n",
        "        self.convs = ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            nn = Sequential(\n",
        "                Linear(channels, channels),\n",
        "                ReLU(),\n",
        "                Linear(channels, channels),\n",
        "            )\n",
        "            conv = MambaConv(channels, GINEConv(nn),shuffle_ind=self.shuffle_ind,d_state=d_state, d_conv=d_conv)\n",
        "            self.convs.append(conv)\n",
        "\n",
        "        # Multi-layer perceptron (MLP) for prediction\n",
        "        self.mlp = Sequential(\n",
        "          Linear(channels, channels // 2),\n",
        "          LeakyReLU(0.2),\n",
        "          Linear(channels // 2, channels // 4),\n",
        "          LeakyReLU(0.2),\n",
        "          Linear(channels // 4, 1),\n",
        "      )\n",
        "\n",
        "    def forward(self, x, pe, edge_index, edge_attr, batch):\n",
        "        # Combine node embeddings and processed pe\n",
        "        x = torch.cat((self.node_emb(x.squeeze(-1)), self.pe_lin(self.pe_norm(pe))), dim=1)\n",
        "        # Process edge attributes throug embedding layer\n",
        "        edge_attr = self.edge_emb(edge_attr)\n",
        "\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index, batch=batch, edge_attr=edge_attr)\n",
        "            # Aggregate node features globally\n",
        "        return self.mlp(global_add_pool(x, batch))"
      ],
      "metadata": {
        "id": "SfV3SHTQgLWB"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.pe, data.edge_index, data.edge_attr, data.batch)\n",
        "        loss = torch.nn.functional.l1_loss(out.squeeze(), data.y, reduction='mean')\n",
        "        loss.backward()\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "        optimizer.step()\n",
        "    return total_loss / len(train_loader.dataset)"
      ],
      "metadata": {
        "id": "m5scM_cWgOgp"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(loader):\n",
        "    model.eval()\n",
        "    total_error = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data.x, data.pe, data.edge_index, data.edge_attr, data.batch)\n",
        "            total_error += torch.nn.functional.l1_loss(out.squeeze(), data.y, reduction='sum').item()\n",
        "    return total_error / len(loader.dataset)\n"
      ],
      "metadata": {
        "id": "4hOp8sPSgQ3R"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = GraphModel(channels=64, pe_dim=8, num_layers=10,\n",
        "                   d_conv=4, d_state=16,shuffle_ind=1\n",
        "                  ).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min')\n",
        "arr = []\n",
        "for epoch in range(1, 6):\n",
        "    loss = train()\n",
        "    val_mae = test(val_loader)\n",
        "    test_mae = test(test_loader)\n",
        "    scheduler.step(val_mae)\n",
        "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_mae:.4f}, '\n",
        "          f'Test: {test_mae:.4f}')\n",
        "    arr.append(test_mae)\n",
        "ordering = arr\n",
        "print(ordering)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ9r7zeKgTSi",
        "outputId": "0688b34e-8e61-493a-db32-f7b12312e4a1"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 0.6294, Val: 0.8125, Test: 0.8713\n",
            "Epoch: 02, Loss: 0.5150, Val: 0.4260, Test: 0.4352\n",
            "Epoch: 03, Loss: 0.4614, Val: 0.4372, Test: 0.4335\n",
            "Epoch: 04, Loss: 0.4495, Val: 0.4455, Test: 0.4531\n",
            "Epoch: 05, Loss: 0.4100, Val: 0.3469, Test: 0.3152\n",
            "[0.871275619506836, 0.43518290519714353, 0.433463321685791, 0.453112211227417, 0.31520862197875976]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WDMp-T64hYST"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}