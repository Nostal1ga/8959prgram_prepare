{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor, matmul\n",
        "from torch.nn import (\n",
        "    BatchNorm1d, Dropout, Embedding, Linear, Module, ModuleList, ReLU, LeakyReLU,Sequential\n",
        ")\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import GNNBenchmarkDataset, Planetoid, ZINC\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import (\n",
        "    GINEConv, global_add_pool, MessagePassing, BatchNorm\n",
        ")\n",
        "from torch_geometric.nn.inits import reset\n",
        "from torch_geometric.nn.resolver import activation_resolver, normalization_resolver\n",
        "from torch_geometric.typing import Adj\n",
        "from torch_geometric.utils import degree, sort_edge_index, to_dense_batch\n",
        "\n",
        "from mamba_ssm import Mamba\n",
        "import inspect"
      ],
      "metadata": {
        "id": "99y6GjJKeEGB"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path, subset = '/temp', True\n",
        "\n",
        "transform = T.AddRandomWalkPE(walk_length=20, attr_name='pe')\n",
        "train_dataset = ZINC(path, subset=subset, split='train', pre_transform=transform)\n",
        "val_dataset = ZINC(path, subset=subset, split='val', pre_transform=transform)\n",
        "test_dataset = ZINC(path, subset=subset, split='test', pre_transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "2-deA9VYojGQ"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MambaConv(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        conv: MessagePassing,\n",
        "        dropout: float = 0.0,\n",
        "        d_state: int = 16,\n",
        "        d_conv: int = 4,\n",
        "        norm: str = 'batch_norm'\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.channels = channels\n",
        "        self.conv = conv\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.self_attn = Mamba(\n",
        "                d_model=channels,\n",
        "                d_state=d_state,\n",
        "                d_conv=d_conv,\n",
        "                expand=2## Expansion factor\n",
        "        )\n",
        "\n",
        "        self.mlp = Sequential(\n",
        "            Linear(channels, channels * 2),\n",
        "            torch.nn.ReLU(),\n",
        "            Dropout(dropout),\n",
        "            Linear(channels * 2, channels),\n",
        "            Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        self.norm1, self.norm2, self.norm3 = [BatchNorm(channels) for _ in range(3)]\n",
        "        self.norm_with_batch = 'batch' in inspect.signature(self.norm1.forward).parameters\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        if self.conv:\n",
        "            self.conv.reset_parameters()\n",
        "        self.self_attn._reset_parameters()\n",
        "        self.mlp.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
        "        for norm in [self.norm1, self.norm2, self.norm3]:\n",
        "            norm.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj, batch: None, **kwargs):\n",
        "        original_x = x\n",
        "        if self.conv:\n",
        "            conv_out = self.conv(x, edge_index, **kwargs)\n",
        "            x = F.dropout(conv_out, p=self.dropout, training=self.training)\n",
        "            x = x + original_x\n",
        "            if self.norm1:\n",
        "                x = self.norm1(x, batch) if self.norm_with_batch else self.norm1(x)\n",
        "        ## Apply self-attention\n",
        "        dense_x, mask = to_dense_batch(x, batch)\n",
        "        attn_out = self.self_attn(dense_x)[mask]\n",
        "        x = F.dropout(attn_out, p=self.dropout, training=self.training) + original_x\n",
        "        if self.norm2:\n",
        "            x = self.norm2(x, batch) if self.norm_with_batch else self.norm2(x)\n",
        "        mlp_out = self.mlp(x)\n",
        "        out = mlp_out + original_x\n",
        "        if self.norm3:\n",
        "            out = self.norm3(out, batch) if self.norm_with_batch else self.norm3(out)\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j, edge_weight):\n",
        "        return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t, x) :\n",
        "        return matmul(adj_t, x, reduce=self.aggr)\n",
        "\n",
        "    def permute_within_batch(self, x, batch):\n",
        "        permuted_indices = torch.empty_like(batch, dtype=torch.long)\n",
        "        # Iterate over each unique batch index directly\n",
        "        for batch_index in torch.unique(batch):\n",
        "            # Find indices within the current batch\n",
        "            indices = torch.where(batch == batch_index)[0]\n",
        "            # Randomly shuffle\n",
        "            shuffled_indices = indices[torch.randperm(indices.size(0))]\n",
        "            permuted_indices[indices] = shuffled_indices\n",
        "        return x[permuted_indices], batch[permuted_indices]\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "      conv_type = self.conv.__class__.__name__ if self.conv else 'None'\n",
        "      return (f\"{self.__class__.__name__}(\"\n",
        "              f\"channels={self.channels}, \"\n",
        "              f\"d_state={self.d_state}, \"\n",
        "              f\"d_conv={self.d_conv}, \")"
      ],
      "metadata": {
        "id": "dOG-R6IEw5fb"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphModel(torch.nn.Module):\n",
        "    def __init__(self, channels: int, pe_dim: int, num_layers: int, d_state: int, d_conv: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.node_emb = Embedding(28, channels - pe_dim)\n",
        "        self.pe_lin = Linear(20, pe_dim)\n",
        "        self.pe_norm = BatchNorm1d(20)\n",
        "        self.edge_emb = Embedding(4, channels)\n",
        "\n",
        "        self.convs = ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            nn = Sequential(\n",
        "                Linear(channels, channels),\n",
        "                ReLU(),\n",
        "                Linear(channels, channels),\n",
        "            )\n",
        "            conv = MambaConv(channels, GINEConv(nn),d_state=d_state, d_conv=d_conv)\n",
        "            self.convs.append(conv)\n",
        "\n",
        "        # Multi-layer perceptron (MLP) for prediction\n",
        "        self.mlp = Sequential(\n",
        "          Linear(channels, channels // 2),\n",
        "          LeakyReLU(0.2),\n",
        "          Linear(channels // 2, channels // 4),\n",
        "          LeakyReLU(0.2),\n",
        "          Linear(channels // 4, 1),\n",
        "      )\n",
        "\n",
        "    def forward(self, x, pe, edge_index, edge_attr, batch):\n",
        "        # Combine node embeddings and processed pe\n",
        "        x = torch.cat((self.node_emb(x.squeeze(-1)), self.pe_lin(self.pe_norm(pe))), dim=1)\n",
        "        # Process edge attributes throug embedding layer\n",
        "        edge_attr = self.edge_emb(edge_attr)\n",
        "\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index, batch=batch, edge_attr=edge_attr)\n",
        "            # Aggregate node features globally\n",
        "        return self.mlp(global_add_pool(x, batch))"
      ],
      "metadata": {
        "id": "ZcXvLUqGw9g5"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.pe, data.edge_index, data.edge_attr, data.batch)\n",
        "        loss = torch.nn.functional.l1_loss(out.squeeze(), data.y, reduction='mean')\n",
        "        loss.backward()\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "        optimizer.step()\n",
        "    return total_loss / len(train_loader.dataset)\n"
      ],
      "metadata": {
        "id": "70eiaIl13R0N"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(loader):\n",
        "    model.eval()\n",
        "    total_error = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data.x, data.pe, data.edge_index, data.edge_attr, data.batch)\n",
        "            total_error += torch.nn.functional.l1_loss(out.squeeze(), data.y, reduction='sum').item()\n",
        "    return total_error / len(loader.dataset)\n"
      ],
      "metadata": {
        "id": "HSnvaaqF3kE1"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = GraphModel(channels=64, pe_dim=8, num_layers=10,\n",
        "                   d_conv=4, d_state=16,\n",
        "                  ).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min')\n",
        "arr = []\n",
        "for epoch in range(1, 6):\n",
        "    loss = train()\n",
        "    val_mae = test(val_loader)\n",
        "    test_mae = test(test_loader)\n",
        "    scheduler.step(val_mae)\n",
        "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_mae:.4f}, '\n",
        "          f'Test: {test_mae:.4f}')\n",
        "    arr.append(test_mae)\n",
        "ordering = arr\n",
        "print(ordering)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGSICb2g5pDj",
        "outputId": "9f3282ea-29eb-42cb-9e5f-aa68d5c82d10"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 0.6481, Val: 0.6769, Test: 0.6624\n",
            "Epoch: 02, Loss: 0.5200, Val: 0.4470, Test: 0.4622\n",
            "Epoch: 03, Loss: 0.4752, Val: 0.4034, Test: 0.3503\n",
            "Epoch: 04, Loss: 0.4453, Val: 0.4360, Test: 0.4107\n",
            "Epoch: 05, Loss: 0.4215, Val: 0.6822, Test: 0.6717\n",
            "[0.6623605422973633, 0.46223797035217284, 0.35028810691833495, 0.4106653003692627, 0.6717489128112792]\n"
          ]
        }
      ]
    }
  ]
}